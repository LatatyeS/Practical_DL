{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salary prediction via Deep NLP methods\n",
    "A recent Kaggle competiton (merely 5 years) propose you to find out who is the most well-paid professional\n",
    "\n",
    "We are gonna solve regression task.\n",
    "\n",
    "Competition is available here: https://www.kaggle.com/c/job-salary-prediction\n",
    "\n",
    "![Hobby](https://imgs.xkcd.com/comics/extrapolating.png)\n",
    "\n",
    "In this notebook you will learn\n",
    " - Data preprocessing for NLP or the most annoying part of data scientist's job\n",
    " - Convolutional Neural Networks for texts\n",
    " - Constructing you NN with scripting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Dataset is consists of job data. Most of it is an unstructured text. You should predict annual salary\n",
    "\n",
    "Download here: https://yadi.sk/d/vVEOWPFY3NruT7 or from competition\n",
    "\n",
    "## Main fields\n",
    "\n",
    "Title - A freetext field supplied to us by the job advertiser as the Title of the job ad.  Normally this is a summary of the job title or role.\n",
    "\n",
    "FullDescription - The full text of the job ad as provided by the job advertiser.  Where you see ***s, we have stripped values from the description in order to ensure that no salary information appears within the descriptions.  There may be some collateral damage here where we have also removed other numerics.\n",
    "\n",
    "LocationRaw - The freetext location as provided by the job advertiser.\n",
    "\n",
    "LocationNormalized - Adzuna's normalised location from within our own location tree, interpreted by us based on the raw location.  Our normaliser is not perfect!\n",
    "\n",
    "ContractType - full_time or part_time, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "ContractTime - permanent or contract, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n",
    "\n",
    "Company - the name of the employer as supplied to us by the job advertiser.\n",
    "\n",
    "Category - which of 30 standard job categories this ad fits into, inferred in a very messy way based on the source the ad came from.  We know there is a lot of noise and error in this field.\n",
    "\n",
    "## Desiered field\n",
    "SalaryRaw - the freetext salary field we received in the job advert from the advertiser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12612628</td>\n",
       "      <td>Engineering Systems Analyst</td>\n",
       "      <td>Engineering Systems Analyst Dorking Surrey Sal...</td>\n",
       "      <td>Dorking, Surrey, Surrey</td>\n",
       "      <td>Dorking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12612830</td>\n",
       "      <td>Stress Engineer Glasgow</td>\n",
       "      <td>Stress Engineer Glasgow Salary **** to **** We...</td>\n",
       "      <td>Glasgow, Scotland, Scotland</td>\n",
       "      <td>Glasgow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 35000/annum 25-35K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12612844</td>\n",
       "      <td>Modelling and simulation analyst</td>\n",
       "      <td>Mathematical Modeller / Simulation Analyst / O...</td>\n",
       "      <td>Hampshire, South East, South East</td>\n",
       "      <td>Hampshire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 40000/annum 20-40K</td>\n",
       "      <td>30000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12613049</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Engineering Systems Analyst / Mathematical Mod...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>25000 - 30000/annum 25K-30K negotiable</td>\n",
       "      <td>27500</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12613647</td>\n",
       "      <td>Pioneer, Miser Engineering Systems Analyst</td>\n",
       "      <td>Pioneer, Miser  Engineering Systems Analyst Do...</td>\n",
       "      <td>Surrey, South East, South East</td>\n",
       "      <td>Surrey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Gregory Martin International</td>\n",
       "      <td>Engineering Jobs</td>\n",
       "      <td>20000 - 30000/annum 20-30K</td>\n",
       "      <td>25000</td>\n",
       "      <td>cv-library.co.uk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id                                              Title  \\\n",
       "0  12612628                        Engineering Systems Analyst   \n",
       "1  12612830                            Stress Engineer Glasgow   \n",
       "2  12612844                   Modelling and simulation analyst   \n",
       "3  12613049  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  12613647         Pioneer, Miser Engineering Systems Analyst   \n",
       "\n",
       "                                     FullDescription  \\\n",
       "0  Engineering Systems Analyst Dorking Surrey Sal...   \n",
       "1  Stress Engineer Glasgow Salary **** to **** We...   \n",
       "2  Mathematical Modeller / Simulation Analyst / O...   \n",
       "3  Engineering Systems Analyst / Mathematical Mod...   \n",
       "4  Pioneer, Miser  Engineering Systems Analyst Do...   \n",
       "\n",
       "                         LocationRaw LocationNormalized ContractType  \\\n",
       "0            Dorking, Surrey, Surrey            Dorking          NaN   \n",
       "1        Glasgow, Scotland, Scotland            Glasgow          NaN   \n",
       "2  Hampshire, South East, South East          Hampshire          NaN   \n",
       "3     Surrey, South East, South East             Surrey          NaN   \n",
       "4     Surrey, South East, South East             Surrey          NaN   \n",
       "\n",
       "  ContractTime                       Company          Category  \\\n",
       "0    permanent  Gregory Martin International  Engineering Jobs   \n",
       "1    permanent  Gregory Martin International  Engineering Jobs   \n",
       "2    permanent  Gregory Martin International  Engineering Jobs   \n",
       "3    permanent  Gregory Martin International  Engineering Jobs   \n",
       "4    permanent  Gregory Martin International  Engineering Jobs   \n",
       "\n",
       "                                SalaryRaw  SalaryNormalized        SourceName  \n",
       "0              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  \n",
       "1              25000 - 35000/annum 25-35K             30000  cv-library.co.uk  \n",
       "2              20000 - 40000/annum 20-40K             30000  cv-library.co.uk  \n",
       "3  25000 - 30000/annum 25K-30K negotiable             27500  cv-library.co.uk  \n",
       "4              20000 - 30000/annum 20-30K             25000  cv-library.co.uk  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame.from_csv('Train_rev1.csv', index_col=None)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.ContractType = X.ContractType.astype(str)\n",
    "X.ContractTime = X.ContractTime.astype(str)\n",
    "X.Company = X.Company.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = X['SalaryNormalized'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del X['Id'], X['Title'],  X['SalaryRaw'], X['SourceName'], X['SalaryNormalized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x121a61f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(Y, bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "At this very stage we will distill valuable data out of the dataset\n",
    "\n",
    "First of all - let's remove rare tokens and finalaize our dictionary\n",
    "\n",
    "We count all tokens ever occurred in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FullDescription', 'LocationRaw', 'LocationNormalized', 'ContractType', 'ContractTime', 'Company', 'Category']\n"
     ]
    }
   ],
   "source": [
    "text_columns = list(X.columns)\n",
    "print (text_columns)\n",
    "categorial_colums = ['ContractType',\n",
    " 'ContractTime',\n",
    " 'Company',\n",
    " 'Category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code is below. \n",
    "\n",
    "ember to apply .lower() to all strings before tokenization\n",
    "\n",
    "Consider using tqdm_notebook for not dying  during the iteration process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244768/244768 [00:33<00:00, 7251.06it/s]\n",
      "100%|██████████| 244768/244768 [00:01<00:00, 195160.86it/s]\n",
      "100%|██████████| 244768/244768 [00:01<00:00, 234762.49it/s]\n",
      "100%|██████████| 244768/244768 [00:00<00:00, 250870.21it/s]\n",
      "100%|██████████| 244768/244768 [00:00<00:00, 247395.31it/s]\n",
      "100%|██████████| 244768/244768 [00:01<00:00, 204689.08it/s]\n",
      "100%|██████████| 244768/244768 [00:01<00:00, 204651.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|\\d+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "def tokenize(value):\n",
    "    return tokenizer.tokenize(value.lower())\n",
    "\n",
    "for col in text_columns:\n",
    "    for text in tqdm.tqdm(X[col]):\n",
    "        token_counts.update(tokenize(text))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert token_counts.most_common(1)[0][1] in  range(2600000, 2700000)\n",
    "assert len(token_counts) in range(200000, 210000)\n",
    "print('Correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to actually build token dict. We will use only words that occur more than `min_count` times in dataset\n",
    "\n",
    "Fill two mappings id->token and token->id\n",
    "\n",
    "**Minimum id is 2**, because 0 is reserved for padding and 1 is UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "tokens = [w for w, c in token_counts.items() if c > min_count] \n",
    "#<tokens from token_counts keys that had at least min_count occurrences throughout the dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert type(tokens)==list\n",
    "#assert len(tokens)==33497\n",
    "assert 'me' in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_size = len(tokens)+2\n",
    "id_to_word = dict()\n",
    "word_to_id = dict()\n",
    "\n",
    "# <your code here>\n",
    "token_to_id = {t:i+2 for i,t in enumerate(tokens)}\n",
    "\n",
    "id_to_token = {i+2:t for i,t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[id_to_token[42]]==42\n",
    "assert len(token_to_id)==len(tokens)\n",
    "assert 0 not in id_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, UNK=1, PAD=0):\n",
    "    '''This function gets a string array and transforms it to padded token matrix\n",
    "    Remember to:\n",
    "     - Transform a string to list of tokens\n",
    "     - Transform each token to it ids (if not in the dict, replace with UNK)\n",
    "     - Pad each line to max_len'''\n",
    "    token_matrix = []\n",
    "    \n",
    "    for s in strings:\n",
    "        seq = [token_to_id.get(token,UNK) for token in tokenize(s)]\n",
    "        token_matrix.append(seq)\n",
    "    \n",
    "    max_len = max(map(len,token_matrix))\n",
    "        \n",
    "    # handle empty batch\n",
    "    if max_len == 0:\n",
    "        max_len = 1\n",
    "    \n",
    "    for i in range(len(token_matrix)):\n",
    "        while(len(token_matrix[i]) < max_len):\n",
    "            token_matrix[i].append(PAD)\n",
    "    \n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "test = vectorize([\"Hello, adshkjasdhkas\", \"data\"], token_to_id, 1)\n",
    "assert test.shape==(2,2)\n",
    "assert (test[:,1]==(1,0)).all()\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you successfully completed all tasks by this moment, you ger 3 pts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True deep learning\n",
    "\n",
    "Now we will define our convolutional neural network.\n",
    "\n",
    "We will think about categorical fields as a sequential - but we won't apply CNN to them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=\"\"\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES = \"\"\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialaize some placeholders for data\n",
    "\n",
    "What size it should  be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "placeholders = dict()\n",
    "for col in text_columns:\n",
    "    placeholders[col] = T.matrix(col,dtype='int32')\n",
    "    \n",
    "true_y = T.vector(dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings are vector represetations for tokens. Basically it is just a table where each token (represented by it's id) has a vector representing its sense.\n",
    "\n",
    "It will be learned simultaneously with other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_size = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep nets\n",
    "Below we are gonna define some network architectures corresponding to each input (a column from a source data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dream_neural_net(word_ids, name):\n",
    "    \n",
    "    output = lasagne.layers.InputLayer([None, None], word_ids)\n",
    "    output = lasagne.layers.EmbeddingLayer(output, input_size=dict_size, output_size=embeddings_size)\n",
    "    output = lasagne.layers.DimshuffleLayer(output, [0,2,1])\n",
    "    #output = lasagne.layers.Conv1DLayer(output, num_filters=8, filter_size=3, pad='same')\n",
    "    #output = lasagne.layers.MaxPool1DLayer(output, pool_size=3)\n",
    "    #output = lasagne.layers.GlobalPoolLayer(output)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dream_neural_net_categorial(word_ids, name):\n",
    "    output = lasagne.layers.InputLayer([None, None], word_ids)\n",
    "    output = lasagne.layers.EmbeddingLayer(output, input_size=dict_size, output_size=embeddings_size)\n",
    "    output = lasagne.layers.DimshuffleLayer(output, [0,2,1])\n",
    "    #output = lasagne.layers.DenseLayer(output, num_units=8, nonlinearity=T.nnet.relu)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to combine all architectures. In a dict below you can match input type and an architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nets_types = {'FullDescription': dream_neural_net, \n",
    " 'LocationRaw': dream_neural_net, \n",
    " 'LocationNormalized': dream_neural_net, \n",
    " 'ContractType': dream_neural_net_categorial, \n",
    " 'ContractTime': dream_neural_net_categorial, \n",
    " 'Company': dream_neural_net_categorial, \n",
    " 'Category':dream_neural_net_categorial\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_to_concat = [nets_types[name](word_ids, name) for name, word_ids in placeholders.items()]\n",
    "dense_repr = lasagne.layers.ConcatLayer(outputs_to_concat, axis=1)\n",
    "#[print(name, word_ids) for name, word_ids in placeholders.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((None, 50, None), (None, 50, None))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_to_concat[0].output_shape, outputs_to_concat[5].output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-693148e465e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_repr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert len(dense_repr.output_shape)==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_to_number(dense_inputs):\n",
    "    <YOUR CODE HERE>\n",
    "    output = lasagne.layers.DenseLayer(output,<YOUR CODE HERE>)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net_output = reduce_to_number(dense_repr)\n",
    "predicted_y = lasagne.layers.get_output(net_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_y = predicted_y.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Remember - we have a regression task, what would be the loss?\n",
    "\n",
    "Also we will estimate a target metric for a competition - **Mean absolute error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = T.mean((predicted_y - true_y)**2)\n",
    "mean_abs_error = T.mean(T.abs_(predicted_y - true_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = lasagne.updates.adam(loss, lasagne.layers.get_all_params(net_output)) # <your code here>\n",
    "\n",
    "train_op = theano.function(list(placeholders.values())+[true_y], [loss, mean_abs_error], updates=optimizer )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_op = theano.function(list(placeholders.values())+[true_y], [loss, mean_abs_error] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process\n",
    "\n",
    "The last thing before we can run the whole monster - define train process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_batches(X, Y=None):\n",
    "    \"\"\"Takes a part of pandas DF\n",
    "    Returns a pair or only X_batch, where X_batch is a dict {key: value} - where a key is name of column, \n",
    "    and a value is a matrix which will be passed to corresponding input\n",
    "    \"\"\"\n",
    "    size = len(X)\n",
    "    i = 0\n",
    "    while i < len(X):\n",
    "        <YOUR CODE HERE>\n",
    "        \n",
    "        if Y is None:\n",
    "            yield X_batch \n",
    "        else:\n",
    "            yield X_batch, Y_batch\n",
    "        i+=batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs(X_batch,Y_batch=None):\n",
    "    feed_dct = [X_batch[k].astype('int32') for k in text_columns]\n",
    "    if Y_batch is not None:\n",
    "        feed_dct.append(Y_batch.astype('int32'))\n",
    "    return feed_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    MSE, AE = 0, 0\n",
    "    batches = 0\n",
    "    <YOUR CODE HERE>\n",
    "    MSE/=batches\n",
    "    AE/=batches\n",
    "    return (MSE, AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X):\n",
    "    return sess.run(predicted_y, get_dict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for first_batch in iterate_batches(X_train, Y_train):\n",
    "    break\n",
    "assert len(first_batch) == 2\n",
    "assert type(first_batch[0]) == dict\n",
    "assert first_batch[1].shape[0]==batch_size\n",
    "assert np.unique([inp.shape[0] for inp in first_batch[0].values()])==batch_size\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE, AE = validate()\n",
    "assert MSE < 1e10\n",
    "assert AE < 50000\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_batches = len(X_train)/batch_size\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Finally it is time to run training\n",
    "\n",
    "First, shuffle the data\n",
    "\n",
    "\n",
    "By the way, if the trainig processes starts and you achive at leasrt **20k** AE error on validation, you get additional **(3 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MSE, AE = validate()\n",
    "assert AE < 20000\n",
    "print (\"I earned 3 pts with %s absolute error!\" % AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = np.random.permutation(len(X_train))\n",
    "X_train = X_train.iloc[p]\n",
    "Y_train = Y_train.iloc[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in range(num_epochs):\n",
    "    for i, (X_batch, Y_batch) in  enumerate(iterate_batches(X_train, Y_train)):\n",
    "        current_loss, abs_error =  train_op(*get_inputs(X_batch,Y_batch))\n",
    "        print \"Current step: %s. Current loss is %s. Absolute error is %s\" % (j, current_loss, abs_error)\n",
    "        if i%30==0:\n",
    "            print(\"Validation. MSE: %s, AE: %s\" % validate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Further experiments and home assignment\n",
    " - **(1 pt)** Add visualisation of train and val loss. Try to use some smoothing to make plots more readable \n",
    " - **(1 pt)** Try different CNN architectures. Vary kernel size, number of filters. Find out if there is some change to a training process.\n",
    " - Try to use different architectures for different inputs. Maybe a smaller architecture would be fine for description field and more complex for a title.\n",
    " - Find out the best **embedding size** value \n",
    " - Add dropout, and some regularisation \n",
    " \n",
    " **(2++ pts) for all experimenting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See in the next series\n",
    " - Recurrent neural networks\n",
    "  - Why everybody like them\n",
    "  - Why everybody hate them\n",
    " - Attention for text processing\n",
    "  - How to boost your NLP model performance by implementing recent DL paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
